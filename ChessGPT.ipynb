{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBidAl6OgOtzsQVwReB63+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tailuge/experiments/blob/master/ChessGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ChessGPT**\n",
        "\n",
        "This is a copy of Andrej Karpathy's colab: \n",
        "\n",
        "https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing\n",
        "\n",
        "I load in some example data from the lichess database, clean it up a bit and then see if a transformer neural network model can predict moves.\n",
        "\n",
        "I'm sure much better ideas exist but it *seems* to work.\n",
        "\n",
        "Produces plausable looking move notation but on small training sets moves are mostly illegal. If you have some good hardware please fork the project and report back with wonder."
      ],
      "metadata": {
        "id": "7eE0TdtH5RHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch a small amount of training data from lichess and clean it up\n",
        "\n",
        "!apt install zstd\n",
        "!curl -r 0-1000000 https://database.lichess.org/standard/lichess_db_standard_rated_2023-03.pgn.zst --output raw.pgn.zst\n",
        "!unzstd raw.pgn.zst --stdout > all.txt\n",
        "!cat all.txt | sed '$ d' | grep -E \"^1. \" | sed 's/{[^}]*}//g' | sed -r 's/[0-9]+[.]+ //g' | sed 's/  / /g' | sed 's/[!?]*//g' | sed 's/ [012/-]*$//' | grep -v '*' > input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPBY-tv-I1kv",
        "outputId": "3834860c-ebeb-43fe-dff5-66a1c86ac258"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "zstd is already the newest version (1.4.4+dfsg-3ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  976k  100  976k    0     0  4787k      0 --:--:-- --:--:-- --:--:-- 4787k\n",
            "raw.pgn.zst : Read error (39) : premature end \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! head input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMPWYeJuXOSJ",
        "outputId": "03ea120c-1568-489a-9920-ac5036e77527"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e4 e5 Nc3 Nf6 Bc4 c6 Nf3 d5 exd5 cxd5 Qe2 Nc6 Bb5 Bd7 Nxe5 d4 Nxc6+ Be7 Nxd8 Kxd8 Na4 Bf5 O-O Bxc2 b3 h5 d3 a6 Nb6 g5 Re1 Nd5 Nxd5 Bf6 Bb2 Bxd3 Qxd3 axb5 Bxd4 Bxd4 Nb6 Ra6 Qxd4+ Kc7 Rac1+ Kb8 Qxh8+ Ka7 Qd4 f5 Nd7+ Ka8 Rc8#\n",
            "d4 d5 c4 dxc4 Nc3 Nc6 e3 Nf6 Bxc4 e5 d5 Nb4 d6 Qxd6 Qxd6 Bxd6 Bb3 Be6 Rb1 Bxb3 axb3 Nc2+ Ke2 O-O e4 Nd4+ Kd1 Bb4 Nge2 Rad8 Bg5 Nxb3+ Nd5 Nxd5 Bxd8 Rxd8 exd5 Rxd5+ Kc2 Nd4+ Nxd4 Rxd4 Rbd1 Rc4+ Kb3 Rg4 f3 Rf4 g3 Rd4 Rxd4 exd4 Kxb4 b5 Rd1 c5+ Kxc5 a5 Rxd4 h5 Kxb5\n",
            "e4 e5 Nf3 d6 Bc4 Bg4 d3 c6 a4 Qf6 Nbd2 h6 b4 a5 b5 Qg6 O-O Bxf3 Nxf3 Qf6 c3 Ne7 d4 Nd7 bxc6 Nxc6 Bd5 Be7 Rb1 Ra7 Bxc6 bxc6 Be3 Rc7 Qd2 O-O c4 Ra8 d5 c5 Qe1 Qg6 Bd2 Rca7 Rb5 Bd8 Nh4 Bxh4 Bxa5 Bd8 Bxd8 Rxd8 a5 Rda8 Qd2 Qxe4 Qa2 Ra6 Ra1 Nb6 Rxb6 Rxb6 Qb1 Rxb1+\n",
            "e4 e6\n",
            "e4 e6 d4 d5 Bd3 dxe4 Bxe4 Nf6 Bf3 Nbd7 c4 Be7 Ne2 Rb8 O-O O-O Nbc3 b6 Nb5 a6 Nbc3 Bb7 d5 Ne5 Nf4 Nxf3+ Qxf3 exd5 cxd5 h6 Rd1 Bd6 Ne4 Re8 Nxd6 Qxd6 Qb3 g5 Nh3 Bxd5 Qc2 Be4 Qb3 Qc6 Bxg5 hxg5 Nxg5 Bd5 Qg3 Qd6 Qh4 Qe5 f4 Qe3+ Kh1 Qe2 Rxd5 Re7 Rdd1 Qg4 Qh6 Rbe8 h3 Qf5 Rf1 Re2 Rg1 Qg6 Qh4 Rxb2 f5 Qg7 Raf1 Rbe2 Qd4 Nh5 Qg4 Nf6 Qh4 Ne4 Nxe4 R2xe4 Rf4 Rxf4 Qxf4 f6 g4 Qe7 Kh2 c5 Rg2 Qe5 Qxe5 Rxe5 Kg3 c4 Rc2 b5 Rd2 Rc5 Rd8+ Kf7 Rd7+ Ke8 Ra7 Rc6 Ra8+ Ke7 Rh8 Kd6 Rh6 Ke5 g5 Kxf5 gxf6 Rc7 Rh5+ Kxf6\n",
            "e4 c5 Nf3 Nc6 Bc4 e6 O-O d5 exd5 exd5 Be2 Nf6 Re1 Be7 Nc3 O-O d4 cxd4 Nxd4 Bc5 Nf3 Re8 h3 Bf5 Bg5 h6 Bh4 g5 Bg3 Nh5 Bh2 g4 hxg4 Bxg4 Nxd5 Nf6 Nc7 Nd7 Nxe8 Qxe8 Bd3 Qc8 Be2 Be7 Qd5 Nf6 Qc4 Be6 Qf4 Qf8 Qc7 Bc8 Bc4 Ne8 Qg3+ Ng7 Nh4 Bg5 f4 Bf6 Kh1 Ne7 Ng6 Nef5 Qb3 Qc5 Bxf7+ Kh7 Bg1 Bd4 Bxd4 Nxd4 Qg3 Qh5+ Kg1 Nxc2 Re5 Bf5 Rc1 Rc8 Rxf5 Qxf5\n",
            "d4 e6 e4 c6 Nc3 d5 e5 c5 Nf3 cxd4 Nxd4 Nc6 Nf3 Qc7 Bf4 Nge7 Bd3 Ng6 Qd2 Bb4 g3 Bxc3 Qxc3 Nxf4 gxf4 O-O Bxh7+ Kh8 Ng5 Qa5 Bd3 Qxc3+ bxc3 Bd7 O-O b6 Rae1 Na5 Re3 Rac8 Rh3+ Kg8 Bh7+ Kh8 Bg6+ Kg8 Bxf7+ Rxf7 Rh8+ Kxh8 Nxf7+ Kg8 Re1 Rxc3 Nd6 Rxc2 Re3 Rc1+ Kg2 Nc4 Rb3 Nxd6 exd6 Rc6 h4 Rxd6 h5 Kh7 Kg3 d4 Rd3 Bb5 Rd2 d3 Kh4 Rd4 Kg5 Bc4 a3 b5 f3 Rd5+ f5 Rxf5+ Kg4 Kh6 f4 Rd5 Kf3 Rxh5 Ke4 Rd5 Ke3 Kg6 Rg2+ Kf6 Kd2 Rd4 Ke3 Rd7 Rh2 g6 Kd2 Rd4 Ke3 Rd5 Rd2 Rf5 Rb2 a6 Rh2 e5 fxe5+ Rxe5+ Kd4 Re1 Rd2 Ra1 Rf2+ Kg7 Rd2 Rxa3 Rg2 a5 Rd2 a4 Rd1 Ra2 Kc3 Rc2+ Kd4 a3 Kc5 a2 Kc6 Rb2 Rf1 Rb1 Rf4 a1=Q Kc5 Qa3+ Kb6 Qd6+ Ka5 Qxf4 Ka6 Qd2 Kb7 Qa5 Kc6 Qb4 Kd7 Re1 Kc6 Re6+ Kd7 Qe7+ Kc8 Qf7\n",
            "e4 b6 d4 Bb7 Nc3 d6 f4 Nd7 Nf3 Ngf6 e5 dxe5 fxe5 Nd5 Nxd5 Bxd5 c3 f6 Qe2 fxe5 Nxe5 Nxe5 Qxe5 Qd6 Bf4 O-O-O Qxd6 exd6 O-O-O Re8 Bg3 g6 Re1 Bh6+ Kb1 Bd2 Rxe8+ Rxe8 Bf2 Be1 Bg3 Bxg3 hxg3 Re1+ Kc2 Re7 Ba6+ Kd7 Rh2 Bxa2 b3 Re2+ Kd3 Rf2 Bc4 Bb1+ Ke3 Rf7 Bxf7 h5 Bc4 c6 Bd3 Ba2 Bc2 d5 Rh1 c5 dxc5 bxc5 Ra1 d4+ cxd4 cxd4+ Kxd4 Bxb3 Bxb3 Kd6 Rxa7\n",
            "e4 Nf6 e5 Nd5 c4 Nb6 Nc3 d6 Nf3 dxe5 Nxe5 Bf5 Qf3 e6 g4 Nxc4 Nxc4 Bc2 Qxb7 Nd7 Bg2 Rb8 Qf3 Bc5 O-O Rg8 Ne4 Bd4 Nc3 Ne5 Nxe5 Bxe5 Qc6+ Kf8 Re1 Bd6 Qe4 Qh4 Qf3 Qxh2+ Kf1 Rb4 Qh3 Qxh3 Bxh3 Bd3+ Kg2 g6 f3 h5 Ne4 hxg4 Bxg4 Rh8 Nxd6 cxd6 Bh3 Bf5 a3 Rbh4 Bxf5 gxf5 Kg3 Rh2 d4 R8h3+ Kf4 Kg7 Be3 Kf6 Bf2 Rg2 Be3 Rh4#\n",
            "e4 e5 Nf3 Nc6 d4 exd4 Nxd4 d6 Be3 Nf6 Nc3 Be7 Qd2 a5 O-O-O Nb4 a3 Nc6 f3 Be6 g4 Nxd4 Bxd4 Qd7 h4 h6 Bd3 d5 e5 Nh7 Bxh7 Rxh7 g5 hxg5 hxg5 Rxh1 Rxh1 O-O-O Rh7 c5 Bf2 d4 Ne4 Qd5 Qxa5 Kb8 Rxg7 d3 c3 Qa2 Nxc5 Qa1+ Kd2 Qxb2+ Ke3 Qe2+ Kf4 Qxf2 Nxe6 fxe6 Rxe7 Qh4+ Ke3 Qxg5+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTyj0bWEYa-5",
        "outputId": "ce244db4-75e5-4d4a-c103-2ab9cda48dcf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  804797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSdimuRmY5_x",
        "outputId": "0e207eca-d546-4537-ed91-5a1a7b556156"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " #+-12345678=BKNOQRabcdefghx\n",
            "29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 16 # what is the maximum context length for predictions?\n",
        "max_iters = 2000\n",
        "eval_interval = 100\n",
        "learning_rate = 5e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 50\n",
        "n_embd = 8\n",
        "n_head = 8\n",
        "n_layer = 8\n",
        "dropout = 1/8\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters()), 'parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-86L0HiibsGd",
        "outputId": "20f7c70a-ad48-425e-cbac-c2c25eec0a73"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7421 parameters\n",
            "step 0: train loss 3.6027, val loss 3.6025\n",
            "step 100: train loss 2.0051, val loss 2.0125\n",
            "step 200: train loss 1.7888, val loss 1.7909\n",
            "step 300: train loss 1.7414, val loss 1.7485\n",
            "step 400: train loss 1.7134, val loss 1.7153\n",
            "step 500: train loss 1.6980, val loss 1.7031\n",
            "step 600: train loss 1.6958, val loss 1.6959\n",
            "step 700: train loss 1.6748, val loss 1.6716\n",
            "step 800: train loss 1.6623, val loss 1.6406\n",
            "step 900: train loss 1.6542, val loss 1.6494\n",
            "step 1000: train loss 1.6397, val loss 1.6378\n",
            "step 1100: train loss 1.6261, val loss 1.6316\n",
            "step 1200: train loss 1.6245, val loss 1.6260\n",
            "step 1300: train loss 1.6075, val loss 1.6265\n",
            "step 1400: train loss 1.6032, val loss 1.6124\n",
            "step 1500: train loss 1.6150, val loss 1.6016\n",
            "step 1600: train loss 1.6008, val loss 1.6191\n",
            "step 1700: train loss 1.6165, val loss 1.6030\n",
            "step 1800: train loss 1.6096, val loss 1.5918\n",
            "step 1900: train loss 1.5994, val loss 1.5981\n",
            "step 1999: train loss 1.6056, val loss 1.5855\n",
            "\n",
            "cxe5 Nd3 d4 Ne6 Qxd1 d4 d3 Bc6 e4 d4 Nxb6 Bf6 Nc3 Rxg8 Ne5+ Rxd7 c1 Bh2 Qxf6 c3 Bxe4 e6 Bxb3 Nb6 N6 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"./s.txt\")"
      ],
      "metadata": {
        "id": "p9iuVy6qQ5ZU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loadedModel = BigramLanguageModel()\n",
        "loadedModel.load_state_dict(torch.load(\"./s.txt\"))\n",
        "loadedModel.eval()\n",
        "\n",
        "# generate from the model\n",
        "loadedm = loadedModel.to(device)\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(loadedm.generate(context, max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qjh6ESX9SGT2",
        "outputId": "356d3503-3ae6-4745-bea7-cb3875f38918"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "f5 Nd7 Qxd6 Qf3 c4 Ne6 exb2 e4 Bxc4 cxb6 Be4 Rxg5 Qxd2+ Kxg1 Ra3 Bxa6+ Ra2 Ke4 h3 0-1\n",
            "e5 Bf3 d4?Qxc7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-emil55Razo",
        "outputId": "92d3e2f0-e331-48d6-e755-780fd6661cc4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "xe4 dxe6 Bxd3 e6 Nxd4 Bxf3 d5 Bxg5 g4 dxg5 fxf2 cxd5 axd6 Nxc7+ Qxf5 Qb3 Nf7 Rxg8 Be4 Rdebc8-O Ba5 Q\n"
          ]
        }
      ]
    }
  ]
}