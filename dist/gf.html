<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Transformer Autoregressive Training Demo</title>
    <!-- TensorFlow.js -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js"></script>
    <!-- Chart.js for plotting -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
      body { font-family: Arial, sans-serif; margin: 20px; }
      #container { max-width: 800px; margin: auto; }
      textarea { width: 100%; height: 120px; margin-bottom: 10px; }
      canvas { border: 1px solid #ccc; margin-top: 10px; }
      button { margin-right: 5px; }
    </style>
  </head>
  <body>
    <div id="container">
      <h1>Transformer Autoregressive Training Demo</h1>
      <textarea id="inputText">
1. e4 e5 2. f4 exf4 3. Nf3 g5 4. h4 g4 5. Ne5
1. e4 e5 2. f4 exf4 3. Nf3 Be7 4. Bc4 Bh4+ 5. g3
1. e4 e5 2. f4 exf4 3. Bc4 Qh4+ 4. Kf1 b5 5. Bxb5
1. e4 e5 2. f4 d5 3. exd5 e4 4. d3 Nf6 5. dxe4 Nxe4
1. e4 e5 2. f4 d5 3. exd5 c6 4. dxc6 Nxc6 5. Nf3 exf4
1. e4 e5 2. f4 d5 3. exd5 Qxd5 4. Nc3 Qd8 5. Nf3 exf4
      </textarea>
      <div>
        <button id="trainButton">Train</button>
        Epochs: <input type="number" id="epochsInput" value="10" min="1">
        <button id="inferButton">Infer</button>
        Prefix: <input type="text" id="prefixInput" value="1. e4">
        <span id="inferenceOutput"></span>
      </div>
      <div>
        <button id="saveButton">Save Model</button>
        <button id="loadButton">Load Model</button>
      </div>
      <!-- Canvas for loss chart -->
      <canvas id="lossCanvas" width="800" height="400"></canvas>
    </div>
    <script>
      // UI Elements
      const trainButton = document.getElementById('trainButton');
      const inferButton = document.getElementById('inferButton');
      const inputText = document.getElementById('inputText');
      const epochsInput = document.getElementById('epochsInput');
      const prefixInput = document.getElementById('prefixInput');
      const inferenceOutput = document.getElementById('inferenceOutput');
      const lossCanvas = document.getElementById('lossCanvas');
      const saveButton = document.getElementById('saveButton');
      const loadButton = document.getElementById('loadButton');

      let model;
      let charToIndex = {};
      let indexToChar = {};
      let vocabSize;
      let allLosses = [];
      // Plot raw loss values.
      const useLogScale = false;
      const seqLength = 10;

      // ---- Chart.js Setup ----
      let lossChart;
      function initChart() {
        const ctx = lossCanvas.getContext('2d');
        lossChart = new Chart(ctx, {
          type: 'line',
          data: {
            labels: [],
            datasets: [{
              label: 'Loss',
              data: [],
              borderColor: 'blue',
              fill: false
            }]
          },
          options: {
            responsive: true,
            scales: {
              x: { title: { display: true, text: 'Epoch' } },
              y: { title: { display: true, text: 'Loss' } }
            }
          }
        });
      }
      function updateChart(losses) {
        lossChart.data.labels = losses.map((_, i) => i + 1);
        lossChart.data.datasets[0].data = losses;
        lossChart.update();
      }
      initChart();

      // ---- Positional Encoding Function ----
      function positionalEncoding(length, depth) {
        const pos = tf.range(0, length);
        const i = tf.range(0, depth);
        const angleRates = tf.div(1, tf.pow(10000, tf.div(tf.cast(tf.floorDiv(i, 2), 'float32'), depth)));
        const posExpanded = pos.expandDims(1);
        const angleRads = tf.mul(posExpanded, angleRates);
        const sinPart = tf.sin(angleRads.slice([0, 0], [length, Math.floor(depth/2)]));
        const cosPart = tf.cos(angleRads.slice([0, Math.floor(depth/2)], [length, depth - Math.floor(depth/2)]));
        return tf.concat([sinPart, cosPart], 1).expandDims(0); // shape [1, length, depth]
      }

      // ---- Causal Mask ----
      function createCausalMask(seqLen) {
        const ones = tf.ones([seqLen, seqLen], 'float32');
        const mask = tf.linalg.bandPart(ones, -1, 0);
        return mask.expandDims(0);
      }

      // ---- Custom Positional Encoding Layer ----
      class PositionalEncodingLayer extends tf.layers.Layer {
        constructor(config) {
          super(config);
          this.seqLen = config.seqLen;
          this.dModel = config.dModel;
        }
        computeOutputShape(inputShape) {
          return inputShape;
        }
        call(inputs, kwargs) {
          return tf.tidy(() => {
            const posEnc = positionalEncoding(this.seqLen, this.dModel);
            // Use tf.add instead of inputs.add to ensure compatibility.
            return tf.add(inputs, posEnc);
          });
        }
        getClassName() {
          return 'PositionalEncodingLayer';
        }
      }

      // ---- Custom Multi-Head Attention Layer ----
      class MultiHeadAttentionLayer extends tf.layers.Layer {
        constructor(config) {
          super(config);
          this.numHeads = config.numHeads;
          this.dModel = config.keyDim; // total embedding dim
          if (this.dModel % this.numHeads !== 0) {
            throw new Error("dModel must be divisible by numHeads");
          }
          this.depth = this.dModel / this.numHeads;
          this.wq = tf.layers.dense({ units: this.dModel });
          this.wk = tf.layers.dense({ units: this.dModel });
          this.wv = tf.layers.dense({ units: this.dModel });
          this.dense = tf.layers.dense({ units: this.dModel });
        }
        splitHeads(x) {
          return tf.tidy(() => {
            const [batch, seqLen, dModel] = x.shape;
            let reshaped = x.reshape([batch, seqLen, this.numHeads, this.depth]);
            return reshaped.transpose([0, 2, 1, 3]); // [batch, numHeads, seqLen, depth]
          });
        }
        scaledDotProductAttention(q, k, v, mask) {
          return tf.tidy(() => {
            const matmulQK = tf.matMul(q, k, false, true); // [batch, numHeads, seqLen, seqLen]
            const dk = tf.scalar(k.shape[k.shape.length - 1], 'float32');
            let scaled = tf.div(matmulQK, tf.sqrt(dk));
            if (mask != null) {
              scaled = scaled.add(mask.mul(-1e9));
            }
            const attnWeights = tf.softmax(scaled, -1);
            return tf.matMul(attnWeights, v); // [batch, numHeads, seqLen, depth]
          });
        }
        call(inputs, kwargs) {
          return tf.tidy(() => {
            const query = inputs.query;
            const key = inputs.key;
            const value = inputs.value;
            const mask = inputs.attentionMask || null;
            const q = this.wq.apply(query);
            const k = this.wk.apply(key);
            const v = this.wv.apply(value);
            const qSplit = this.splitHeads(q);
            const kSplit = this.splitHeads(k);
            const vSplit = this.splitHeads(v);
            const attnOutput = this.scaledDotProductAttention(qSplit, kSplit, vSplit, mask);
            const attnTransposed = attnOutput.transpose([0, 2, 1, 3]);
            const concatAttn = attnTransposed.reshape([attnTransposed.shape[0], attnTransposed.shape[1], this.dModel]);
            return this.dense.apply(concatAttn);
          });
        }
        computeOutputShape(inputShape) {
          return inputShape.query;
        }
        getClassName() {
          return 'MultiHeadAttentionLayer';
        }
      }

      // ---- Custom Transformer Block Layer ----
      class TransformerBlockLayer extends tf.layers.Layer {
        constructor(config) {
          super(config);
          this.numHeads = config.numHeads;
          this.dff = config.dff;
          this.dModel = config.keyDim; 
          this.mha = new MultiHeadAttentionLayer({ numHeads: this.numHeads, keyDim: this.dModel });
          this.layerNorm1 = tf.layers.layerNormalization();
          this.ffnDense1 = tf.layers.dense({ units: this.dff, activation: 'relu' });
          this.ffnDense2 = tf.layers.dense({ units: this.dModel });
          this.layerNorm2 = tf.layers.layerNormalization();
        }
        computeOutputShape(inputShape) {
          return inputShape;
        }
        call(inputs, kwargs) {
          return tf.tidy(() => {
            const x = inputs;
            const currentSeqLen = x.shape[1];
            const mask = createCausalMask(currentSeqLen);
            const attnOutput = this.mha.apply({ query: x, key: x, value: x, attentionMask: mask });
            const out1 = this.layerNorm1.apply(tf.add(x, attnOutput));
            const ffnOutput = this.ffnDense2.apply(this.ffnDense1.apply(out1));
            const out2 = this.layerNorm2.apply(tf.add(out1, ffnOutput));
            return out2;
          });
        }
        getClassName() {
          return 'TransformerBlockLayer';
        }
      }

      // ---- Build Transformer Model ----
      function buildTransformerModel(vocabSize, embeddingDim, seqLen, numLayers, numHeads, dff) {
        const model = tf.sequential();
        model.add(tf.layers.embedding({ inputDim: vocabSize, outputDim: embeddingDim, inputLength: seqLen }));
        model.add(new PositionalEncodingLayer({ seqLen: seqLen, dModel: embeddingDim }));
        for (let i = 0; i < numLayers; i++) {
          model.add(new TransformerBlockLayer({ numHeads: numHeads, dff: dff, keyDim: embeddingDim }));
        }
        model.add(tf.layers.dense({ units: vocabSize, activation: 'softmax' }));
        return model;
      }

      // ---- Data Preprocessing ----
      function preprocessData(text) {
        const lines = text.trim().split('\n');
        const allChars = [...new Set(lines.join(''))].sort();
        charToIndex = {};
        indexToChar = {};
        allChars.forEach((char, i) => {
          charToIndex[char] = i;
          indexToChar[i] = char;
        });
        vocabSize = allChars.length;
        return lines.map(line => line.split('').map(ch => charToIndex[ch]));
      }
      
      function createDataset(sequences, seqLength) {
        const inputs = [];
        const targets = [];
        sequences.forEach(seq => {
          for (let i = 0; i < seq.length - seqLength; i++) {
            inputs.push(seq.slice(i, i + seqLength));
            targets.push(seq.slice(i + 1, i + seqLength + 1));
          }
        });
        const xs = tf.tensor2d(inputs, [inputs.length, seqLength], 'int32');
        const ysIndices = tf.tensor2d(targets, [targets.length, seqLength], 'int32');
        const ys = tf.oneHot(ysIndices, vocabSize);
        ysIndices.dispose();
        return [xs, ys];
      }
      
      // ---- Training and Inference ----
      async function trainModel(epochs) {
        const text = inputText.value;
        const sequences = preprocessData(text);
        const [xs, ys] = createDataset(sequences, seqLength);
        
        if (!model) {
          const embeddingDim = 32, numLayers = 2, numHeads = 4, dff = 64;
          model = buildTransformerModel(vocabSize, embeddingDim, seqLength, numLayers, numHeads, dff);
          model.compile({
            optimizer: tf.train.adam(0.01),
            loss: 'categoricalCrossentropy',
            metrics: ['accuracy']
          });
        }
        
        await model.fit(xs, ys, {
          epochs: epochs,
          batchSize: 32,
          shuffle: true,
          callbacks: {
            onEpochEnd: async (epoch, logs) => {
              console.log(`Epoch ${epoch + 1}: Loss = ${logs.loss.toFixed(4)}`);
              allLosses.push(logs.loss);
              updateChart(allLosses);
            }
          }
        });
        xs.dispose();
        ys.dispose();
      }
      
      async function infer(prefix, numGenerate = 20) {
        if (!model) {
          inferenceOutput.textContent = "Model not trained yet!";
          return;
        }
        const prefixIndices = prefix.split('').map(ch => charToIndex[ch] || 0);
        let inputSeq = prefixIndices.slice();
        let generated = prefix;
        for (let i = 0; i < numGenerate; i++) {
          const inputTensor = tf.tensor2d([inputSeq], [1, inputSeq.length], 'int32');
          const output = model.predict(inputTensor);
          const outputData = await output.array();
          const lastProbs = outputData[0][outputData[0].length - 1];
          const predictedIndex = lastProbs.indexOf(Math.max(...lastProbs));
          generated += indexToChar[predictedIndex] || '';
          inputSeq.push(predictedIndex);
          inputTensor.dispose();
          output.dispose();
        }
        inferenceOutput.textContent = generated;
      }
      
      async function saveModel() {
        if (!model) return;
        await model.save('localstorage://my-model');
        console.log("Model saved to local storage.");
      }
      
      async function loadModel() {
        try {
          model = await tf.loadLayersModel('localstorage://my-model');
          preprocessData(inputText.value);
          model.compile({
            optimizer: tf.train.adam(0.01),
            loss: 'categoricalCrossentropy',
            metrics: ['accuracy']
          });
          console.log("Model loaded from local storage.");
        } catch (error) {
          console.error("Error loading model:", error);
          alert("Failed to load model. Ensure a model has been saved.");
        }
      }
      
      // ---- Event Listeners ----
      trainButton.addEventListener('click', async () => {
        const epochs = parseInt(epochsInput.value, 10);
        await trainModel(epochs);
      });
      
      inferButton.addEventListener('click', () => {
        const prefix = prefixInput.value;
        infer(prefix);
      });
      
      saveButton.addEventListener('click', saveModel);
      loadButton.addEventListener('click', loadModel);
    </script>
  </body>
</html>
